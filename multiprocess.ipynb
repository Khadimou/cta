{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from turtle import color\n",
    "import tables\n",
    "from tables import *\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA \n",
    "import glob \n",
    "import matplotlib.pyplot as plt\n",
    "#import relevant libraries for 3d graph\n",
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import tqdm\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hfile = tables.open_file(\"Data/training/train2.h5\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to retrieve parameter values from cta dataset\n",
    "def get_img(fname):\n",
    "    file = tables.open_file(fname,\"r\")\n",
    "    #visualize images from the telescope\n",
    "    img = file.root.dl1.event.telescope.images.LST_LSTCam.col(\"image\")\n",
    "\n",
    "    return  img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_img(file_offset):\n",
    "    images = get_img(file_offset[0])\n",
    "\n",
    "    file_offset[1][:] = images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n"
     ]
    }
   ],
   "source": [
    "train_fnames = glob.glob('Data/training/*.h5',recursive=True) #500 files\n",
    "nbevent = 0\n",
    "file_offset = []\n",
    "file_view = []\n",
    "\n",
    "for file in train_fnames:\n",
    "    hfile = tables.open_file(file, \"r\")\n",
    "    nrows = hfile.root.dl1.event.telescope.images.LST_LSTCam.nrows\n",
    "    file_offset.append((file,nbevent, nbevent+nrows))\n",
    "    nbevent = nbevent + nrows\n",
    "    hfile.close()\n",
    "\n",
    "tab_event_img =  np.zeros((nbevent,1855))\n",
    "for tab in file_offset:\n",
    "    file_view.append((tab[0], tab_event_img[tab[1]:tab[2]]))\n",
    "with Pool(96) as p:\n",
    "    print(p.map(retrieve_img, file_view))\n",
    "\n",
    "# tab_event_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_event_img.tofile(\"event_img_1058742_1855.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('Data/training/train165.h5', 0, 2196),\n",
       " ('Data/training/train165.h5',\n",
       "  array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]])))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_offset[0] ,file_view[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# principal component analysis on images\n",
    "def pca(img):\n",
    "\n",
    "    #Scale data before applying PCA\n",
    "    scaling = StandardScaler()\n",
    "    data = img.data\n",
    "\n",
    "    #use fit and transform method\n",
    "    scaling.fit(data)\n",
    "    Scaled_data = scaling.transform(data)\n",
    "\n",
    "    #set the n_components\n",
    "    principal = PCA(n_components=0.85)\n",
    "    principal.fit(Scaled_data)\n",
    "    x = principal.transform(Scaled_data)\n",
    "    # Check the values of eigen vectors\n",
    "    # prodeced by principal components\n",
    "    print(\"Principal components \\n\",principal.components_)\n",
    "    # check how much variance is explained by each principal component\n",
    "    explained_variance = principal.explained_variance_ratio_\n",
    "    #print(\"variance : \",explained_variance)\n",
    "    explained_variance = np.insert(explained_variance,0,0) #setting x=0 ; y=0 to make the scree plot\n",
    "\n",
    "    #preparing the cumulative variance data\n",
    "    cumulative_variance = np.cumsum(np.round(explained_variance, decimals=3))\n",
    "    explained_variance_df = pd.DataFrame(explained_variance, columns=['Explained Variance'])\n",
    "    cumulative_variance_df = pd.DataFrame(cumulative_variance, columns=['Cumulative Variance'])\n",
    "\n",
    "    scores = pd.DataFrame(data=x)\n",
    "    names = []\n",
    "    for r in range(5):\n",
    "        names.append( 'PC'+str(r) )\n",
    "    scores.columns = [names]\n",
    "    #print(scores)\n",
    "    pc_df = pd.DataFrame(['','PC0', 'PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10', 'PC11', 'PC12', 'PC13', 'PC14', 'PC15', 'PC16', 'PC17', 'PC18', 'PC19', 'PC20', 'PC21', 'PC22', 'PC23', 'PC24', 'PC25', 'PC26', 'PC27', 'PC28', 'PC29', 'PC30', 'PC31', 'PC32', 'PC33', 'PC34', 'PC35', 'PC36', 'PC37', 'PC38', 'PC39', 'PC40', 'PC41', 'PC42', 'PC43', 'PC44', 'PC45', 'PC46', 'PC47', 'PC48', 'PC49', 'PC50', 'PC51', 'PC52', 'PC53', 'PC54', 'PC55', 'PC56', 'PC57', 'PC58', 'PC59', 'PC60', 'PC61', 'PC62', 'PC63', 'PC64', 'PC65', 'PC66', 'PC67', 'PC68', 'PC69', 'PC70', 'PC71', 'PC72', 'PC73', 'PC74', 'PC75', 'PC76', 'PC77', 'PC78', 'PC79', 'PC80', 'PC81', 'PC82', 'PC83', 'PC84', 'PC85', 'PC86', 'PC87', 'PC88', 'PC89', 'PC90', 'PC91', 'PC92', 'PC93', 'PC94', 'PC95', 'PC96', 'PC97', 'PC98', 'PC99', 'PC100', 'PC101', 'PC102', 'PC103', 'PC104', 'PC105', 'PC106', 'PC107', 'PC108', 'PC109', 'PC110', 'PC111', 'PC112', 'PC113', 'PC114', 'PC115', 'PC116', 'PC117', 'PC118', 'PC119', 'PC120', 'PC121', 'PC122', 'PC123', 'PC124', 'PC125', 'PC126', 'PC127', 'PC128', 'PC129', 'PC130', 'PC131', 'PC132', 'PC133', 'PC134', 'PC135', 'PC136', 'PC137', 'PC138', 'PC139', 'PC140', 'PC141', 'PC142', 'PC143', 'PC144', 'PC145', 'PC146', 'PC147', 'PC148', 'PC149', 'PC150', 'PC151', 'PC152', 'PC153', 'PC154', 'PC155', 'PC156', 'PC157', 'PC158', 'PC159', 'PC160', 'PC161', 'PC162', 'PC163', 'PC164', 'PC165', 'PC166', 'PC167', 'PC168', 'PC169', 'PC170', 'PC171', 'PC172', 'PC173', 'PC174', 'PC175', 'PC176', 'PC177', 'PC178', 'PC179', 'PC180', 'PC181', 'PC182', 'PC183', 'PC184', 'PC185', 'PC186', 'PC187', 'PC188', 'PC189', 'PC190', 'PC191', 'PC192', 'PC193', 'PC194', 'PC195', 'PC196', 'PC197', 'PC198', 'PC199', 'PC200', 'PC201', 'PC202', 'PC203', 'PC204', 'PC205', 'PC206', 'PC207', 'PC208', 'PC209', 'PC210', 'PC211', 'PC212', 'PC213', 'PC214', 'PC215', 'PC216', 'PC217', 'PC218', 'PC219', 'PC220', 'PC221', 'PC222', 'PC223', 'PC224', 'PC225', 'PC226', 'PC227', 'PC228', 'PC229', 'PC230', 'PC231', 'PC232', 'PC233', 'PC234', 'PC235', 'PC236', 'PC237', 'PC238', 'PC239', 'PC240', 'PC241', 'PC242', 'PC243', 'PC244', 'PC245', 'PC246', 'PC247', 'PC248', 'PC249', 'PC250', 'PC251', 'PC252', 'PC253', 'PC254', 'PC255', 'PC256', 'PC257', 'PC258', 'PC259', 'PC260', 'PC261', 'PC262', 'PC263', 'PC264', 'PC265', 'PC266', 'PC267', 'PC268', 'PC269', 'PC270', 'PC271', 'PC272', 'PC273', 'PC274', 'PC275', 'PC276', 'PC277', 'PC278', 'PC279', 'PC280', 'PC281', 'PC282', 'PC283', 'PC284', 'PC285', 'PC286', 'PC287', 'PC288', 'PC289', 'PC290', 'PC291', 'PC292', 'PC293', 'PC294', 'PC295', 'PC296', 'PC297', 'PC298', 'PC299', 'PC300', 'PC301', 'PC302', 'PC303', 'PC304', 'PC305', 'PC306', 'PC307', 'PC308', 'PC309', 'PC310', 'PC311', 'PC312', 'PC313', 'PC314', 'PC315', 'PC316', 'PC317', 'PC318', 'PC319', 'PC320', 'PC321', 'PC322', 'PC323', 'PC324', 'PC325', 'PC326', 'PC327', 'PC328', 'PC329', 'PC330', 'PC331', 'PC332', 'PC333', 'PC334', 'PC335', 'PC336', 'PC337', 'PC338', 'PC339', 'PC340', 'PC341', 'PC342', 'PC343', 'PC344', 'PC345', 'PC346', 'PC347', 'PC348', 'PC349', 'PC350', 'PC351', 'PC352', 'PC353', 'PC354', 'PC355', 'PC356', 'PC357', 'PC358', 'PC359', 'PC360', 'PC361', 'PC362', 'PC363', 'PC364', 'PC365', 'PC366', 'PC367', 'PC368', 'PC369', 'PC370', 'PC371', 'PC372', 'PC373', 'PC374', 'PC375', 'PC376', 'PC377', 'PC378', 'PC379', 'PC380', 'PC381', 'PC382', 'PC383', 'PC384', 'PC385', 'PC386', 'PC387', 'PC388', 'PC389', 'PC390', 'PC391', 'PC392', 'PC393', 'PC394', 'PC395', 'PC396', 'PC397', 'PC398', 'PC399', 'PC400', 'PC401', 'PC402', 'PC403', 'PC404', 'PC405', 'PC406', 'PC407', 'PC408', 'PC409', 'PC410', 'PC411', 'PC412', 'PC413', 'PC414', 'PC415', 'PC416', 'PC417', 'PC418', 'PC419', 'PC420', 'PC421', 'PC422', 'PC423', 'PC424', 'PC425', 'PC426', 'PC427', 'PC428', 'PC429', 'PC430', 'PC431', 'PC432', 'PC433', 'PC434', 'PC435', 'PC436', 'PC437', 'PC438', 'PC439', 'PC440', 'PC441', 'PC442', 'PC443', 'PC444', 'PC445', 'PC446', 'PC447', 'PC448', 'PC449', 'PC450', 'PC451', 'PC452', 'PC453', 'PC454', 'PC455', 'PC456', 'PC457', 'PC458', 'PC459', 'PC460', 'PC461', 'PC462', 'PC463', 'PC464', 'PC465', 'PC466', 'PC467', 'PC468', 'PC469', 'PC470', 'PC471', 'PC472', 'PC473', 'PC474', 'PC475', 'PC476', 'PC477', 'PC478', 'PC479', 'PC480', 'PC481', 'PC482', 'PC483', 'PC484', 'PC485', 'PC486', 'PC487', 'PC488', 'PC489', 'PC490', 'PC491', 'PC492', 'PC493', 'PC494', 'PC495', 'PC496', 'PC497', 'PC498', 'PC499'], columns=['PC'])\n",
    "    df_explainedvariance = pd.concat([pc_df,explained_variance_df,cumulative_variance_df], axis=1).dropna()\n",
    "    print(\"tables of variance \",df_explainedvariance)\n",
    "\n",
    "    # max value for multiple columns\n",
    "    max_val = df_explainedvariance[['Explained Variance']].max()\n",
    "    print(\"Max Variance \",max_val)\n",
    "    #max value and sum of the explained variance\n",
    "    print(\"somme totale variance \",df_explainedvariance[['Explained Variance']].sum())\n",
    "\n",
    "     # explained variance + cumulative variance (separate plot)\n",
    "    fig = make_subplots(rows=3, cols=1)\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_explainedvariance['PC'],\n",
    "            y=df_explainedvariance['Cumulative Variance'],\n",
    "            marker=dict(size=15, color=\"LightSeaGreen\")\n",
    "        ), row=1, col=1\n",
    "        )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=df_explainedvariance['PC'],\n",
    "            y=df_explainedvariance['Explained Variance'],\n",
    "            marker=dict(color=\"RoyalBlue\"),\n",
    "        ), row=3, col=1\n",
    "        )\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "htable = np.fromfile(\"injunction_table_lst.pny\", offset=8, dtype=np.uint16)\n",
    "def get_img_matrix(img, nb_img):\n",
    "    matrix = np.zeros(55*55)\n",
    "\n",
    "    mat = np.zeros((nb_img,55,55),dtype=np.float32)\n",
    "\n",
    "    for i in range(nb_img):\n",
    "        matrix[htable] = img[i]\n",
    "        res = matrix.reshape(55,55)\n",
    "        mat[i] = res\n",
    "\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#return the singular values\n",
    "def svd(res):\n",
    "    # Applying SVD\n",
    "    U, S, VT = np.linalg.svd(res,full_matrices=False,# It's not necessary to compute the full matrix of U or V\n",
    "        compute_uv=True) # Deterministic SVD\n",
    "\n",
    "    S = np.diag(S) #singular value matrix\n",
    "   \n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_svd(file_offset):\n",
    "    file, outputsvd = file_offset\n",
    "    hfile =tables.open_file(file,\"r\")\n",
    "    sv= hfile.root.Shower.gerbe.col(\"sv\")\n",
    "    \n",
    "\n",
    "    outputsvd[:] = sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fnames = glob.glob('hash_training/*.h5',recursive=True) #500 files\n",
    "nbevent = 0\n",
    "file_offset = []\n",
    "file_view = []\n",
    "\n",
    "for file in train_fnames:\n",
    "    hfile = tables.open_file(file, \"r\")\n",
    "    nrows = hfile.root.Shower.gerbe.nrows\n",
    "    file_offset.append((file,nbevent, nbevent+nrows))\n",
    "    nbevent = nbevent + nrows\n",
    "    hfile.close()\n",
    "\n",
    "tab_event_svd =  np.zeros((nbevent,55))\n",
    "for tab in file_offset:\n",
    "    file_view.append((tab[0], tab_event_svd[tab[1]:tab[2]]))\n",
    "with Pool(96) as p:\n",
    "        p.map(call_svd, file_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9714c79cc20781910ed960df3f6fbe0fd0a0039e985b4bcf55af094e2ab2306"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 ('CTA')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
